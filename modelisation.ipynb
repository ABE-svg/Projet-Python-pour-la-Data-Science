{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import io\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold,StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction de téléchargement des données sur les accidents corporels\n",
    "def telecharge(url_data,filename, path):\n",
    "    # Vérifie si le dossier 'data' existe, sinon le crée\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    #télécharge les données avec l'url\n",
    "    response = requests.get(url_data)\n",
    "    if response.status_code == 200:\n",
    "        file_path = os.path.join(path, filename)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Fichier {filename} téléchargé avec succès:{file_path}\")\n",
    "    else:\n",
    "        print(f\"Echec de téléchargement pour {filename}. Statut: {response.status_code}\")\n",
    "\n",
    "# API pour accéder à l'url de téléchargement\n",
    "url_root=\"https://www.data.gouv.fr/api/1/datasets/53698f4ca3a729239d2036df/resources/\"\n",
    "urls={\n",
    "    \"usagers-2023.csv\":\"68848e2a-28dd-4efc-9d5f-d512f7dbe66f\",\n",
    "    \"vehicules-2023.csv\":\"146a42f5-19f0-4b3e-a887-5cd8fbef057b\",\n",
    "    \"lieux-2023.csv\":\"8bef19bf-a5e4-46b3-b5f9-a145da4686bc\",\n",
    "    \"caract-2023.csv\":\"104dbb32-704f-4e99-a71e-43563cb604f2\"\n",
    "}\n",
    "path='data'\n",
    "\n",
    "for filename, resource_id in urls.items():\n",
    "    url=url_root+resource_id\n",
    "    response1=requests.get(url)\n",
    "    if response1.status_code==200:\n",
    "        data=response1.json()\n",
    "        url_data=data['url']\n",
    "    else:\n",
    "        print(\"downloading failed\")\n",
    "    telecharge(url_data,filename,path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base des usagers \n",
    "df_usagers = pd.read_csv(\"data/usagers-2023.csv\", sep = ';')\n",
    "df_usagers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base des véhicules \n",
    "df_vehicules = pd.read_csv(\"data/vehicules-2023.csv\", sep = ';')\n",
    "df_vehicules.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base des lieux \n",
    "df_lieux = pd.read_csv(\"data/lieux-2023.csv\", sep = ';')\n",
    "df_lieux.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caract = pd.read_csv(\"data/caract-2023.csv\", sep = ';')\n",
    "df_caract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df_usagers.merge(df_vehicules, on=[\"Num_Acc\",\"id_vehicule\",\"num_veh\"], how=\"inner\") \n",
    "df_merge = df_merge.merge(df_lieux, on=\"Num_Acc\", how=\"inner\")\n",
    "df_merge = df_merge.merge(df_caract, on=\"Num_Acc\", how=\"inner\")\n",
    "df_merge.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modélisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages nécéssaire\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold,StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul de l'âge des piétons\n",
    "df_merge[\"Age\"] = 2023-df_merge[\"an_nais\"]\n",
    "# Distribution de l'âge\n",
    "df_merge[\"Age\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supression des variables d'identification\n",
    "var=[\"Num_Acc\",\"jour\",\"an\",\"com\",\"adr\",\"lat\",\"long\",\"voie\",\"v1\",\"v2\",\"id_vehicule\",\"num_veh\",\"id_usager\",\"an_nais\", \"dep\"]\n",
    "Num_acc=df_merge[\"Num_Acc\"]\n",
    "df_merge.drop(var, axis=1,inplace=True)\n",
    "\n",
    "# pourcentage des valeurs manquantes par variables \n",
    "df_merge.isna().sum()[df_merge.isna().sum()!=0]/len(df_merge)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.drop([\"occutc\",\"lartpc\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation des valeurs manquantes de l'âge\n",
    "sns.boxplot(data=df_merge, x=df_merge['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge[\"Age\"]=df_merge[\"Age\"].fillna(df_merge[\"Age\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recodage des variables trajet et actp de la base usager\n",
    "df_merge[\"trajet\"]=df_merge[\"trajet\"].replace(0, value=-1)\n",
    "df_merge[\"actp\"]=df_merge[\"actp\"].replace(0, value=-1)\n",
    "\n",
    "# variables de type object de la base\n",
    "df_merge.select_dtypes(\"object\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion des variables de type object en variables numériques\n",
    "def replace_pr(val):\n",
    "    if len(val.split())==1:\n",
    "        val=val\n",
    "    else:\n",
    "        if len(val.split())>1:\n",
    "            val=val.split()[1]\n",
    "        else:\n",
    "            val=\"-1\"\n",
    "    return val\n",
    "df_merge[\"pr\"]=df_merge[\"pr\"].apply(func=replace_pr)\n",
    "df_merge[\"pr\"]=df_merge[\"pr\"].astype(float)\n",
    "df_merge[\"pr1\"]=df_merge[\"pr1\"].apply(func=replace_pr)\n",
    "df_merge[\"pr1\"]=df_merge[\"pr1\"].astype(float)\n",
    "def rep_virgule(val):\n",
    "    val=val.replace(\",\",\".\")\n",
    "    return val\n",
    "df_merge[\"larrout\"]=df_merge[\"larrout\"].apply(rep_virgule)\n",
    "df_merge[\"larrout\"]=df_merge[\"larrout\"].astype(float)\n",
    "def heure(val):\n",
    "    val=val.split(\":\")[0]\n",
    "    return val\n",
    "df_merge[\"hrmn\"]=df_merge[\"hrmn\"].apply(func=heure)\n",
    "df_merge[\"hrmn\"]=df_merge[\"hrmn\"].astype(float)\n",
    "\n",
    "df_merge[\"nbv\"]=df_merge[\"nbv\"].str.strip()\n",
    "df_merge[\"nbv\"]=df_merge[\"nbv\"].replace(\"#VALEURMULTI\",\"-1\")\n",
    "df_merge[\"nbv\"]=df_merge[\"nbv\"].astype(float)\n",
    "\n",
    "df_merge[\"actp\"]=df_merge[\"actp\"].replace([\"A\",\"B\"],[\"10\",\"11\"])\n",
    "df_merge[\"actp\"]=df_merge[\"actp\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppression des colonnes qui ont plus de 20% de valeurs non renseignés\n",
    "diction=[]\n",
    "for column in df_merge.columns:\n",
    "    if ((len(df_merge[df_merge[column]==-1][column])/df_merge.shape[0]*100)>=20) == True:\n",
    "        diction.append(column)\n",
    "        \n",
    "df_merge=df_merge.drop(diction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_merge[[\"secu1\",\"secu2\",\"catu\",\"place\",\"grav\"]]\n",
    "df=df[df[\"secu2\"]!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.isnull().sum()[df_merge.isnull().sum()!=0]/len(df_merge)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# on remplace toute les valeurs non renseignées des variables restantes par le mode\n",
    "diction2=[]\n",
    "for column in df_merge.columns:\n",
    "    if len(df_merge[df_merge[column]==-1][column])/df_merge.shape[0]!=0:\n",
    "        diction2.append(column)\n",
    "\n",
    "for col in diction2:\n",
    "    #mode=df_merge[col].value_counts().idxmax()\n",
    "    df_merge[col]=df_merge[col].replace(-1,np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge[\"place\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relation target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "\n",
    "for col in df.columns:\n",
    "\n",
    "    # Création d'une table de contingence\n",
    "    contingency_table = pd.crosstab(df[col], df[\"grav\"])\n",
    "\n",
    "    # Calcul du chi-deux et du V de Cramer\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    n = contingency_table.sum().sum()\n",
    "    mindim = min(contingency_table.shape)-1\n",
    "\n",
    "    # Calcul du V de Cramer\n",
    "    v = np.sqrt(chi2 / (n * mindim))\n",
    "    print(col)\n",
    "    print('Coefficient cramer:', v)\n",
    "    print('P-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "\n",
    "for col in df_merge.drop([\"Age\",'hrmn','vma',\"nbv\",\"grav\"], axis=1).columns:\n",
    "\n",
    "    # Création d'une table de contingence\n",
    "    contingency_table = pd.crosstab(df_merge[col], df_merge[\"grav\"])\n",
    "\n",
    "    # Calcul du chi-deux et du V de Cramer\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    n = contingency_table.sum().sum()\n",
    "    mindim = min(contingency_table.shape)-1\n",
    "\n",
    "    # Calcul du V de Cramer\n",
    "    v = np.sqrt(chi2 / (n * mindim))\n",
    "    print(col)\n",
    "    print('Coefficient cramer:', v)\n",
    "    print('P-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "for col in [\"Age\",'hrmn','vma',\"nbv\"]:\n",
    "    #dfcol=df[[col,\"grav\"]].dropna()\n",
    "    corr, p_value = pearsonr(df_merge[col], df_merge[\"grav\"])\n",
    "    print(col)\n",
    "    print('Coefficient de corrélation de point bisériel:', corr)\n",
    "    print('P-value:', p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On caractérise chaque accident par le niveau de gravité le plus haut\n",
    "df_merge[\"Num_Acc\"]=Num_acc\n",
    "import numpy as np\n",
    "id_acc=df_merge[\"Num_Acc\"].unique()\n",
    "index_true=[]\n",
    "for i in id_acc:\n",
    "    df_id=df_merge[df_merge[\"Num_Acc\"]==i]\n",
    "    n_grav=np.max(df_id[\"grav\"])\n",
    "    index=df_merge[(df_merge[\"Num_Acc\"]==i) & (df_merge[\"grav\"]==n_grav)].index[0]\n",
    "    #if len(index)!=0:\n",
    "    #df_merge.drop(index=index, inplace=True)\n",
    "    index_true.append(index)\n",
    "        \n",
    "df_merge.drop([\"Num_Acc\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Niveau de gravité de l'accident présente dans la nouvelle base\n",
    "df_merge=df_merge.loc[index_true]\n",
    "df_merge[\"grav\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans les accidents enregistrés, aucun n'a laisser toutes les personnes impliquées indemnes.\n",
    "\n",
    "On recode la variable \"gravité\" par: \n",
    "- 2: niveau de gravité **grave** (2)\n",
    "- 3: niveau de gravité **moyen** (1)\n",
    "- 4: niveau de gravité **faible** (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recodage de la gravité de l'accident\n",
    "df_merge[\"grav\"]=df_merge[\"grav\"].replace([3,4], value=[1,0])\n",
    "# distribution de la gravité\n",
    "df_merge[\"grav\"].value_counts()/df_merge.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sélection des variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la selection des variables qui seront utilisées dans le modèle de régression logistique, nous utiliserons une regression Lasso. C'est une technique de régularisation qui consiste à appliquer une pénalité pour éviter le surapprentissage et améliorer la précision des modèles statistiques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recodage des variables \n",
    "#df_merge[\"trajet\"]=df_merge[\"trajet\"].replace(0, value=-1)\n",
    "#df_merge[\"actp\"]=df_merge[\"actp\"].replace(0, value=-1)\n",
    "\n",
    "df_merge[\"catv\"] = df_merge[\"catv\"].replace([4,30,32,2,5,31,33,34],value=1)\n",
    "df_merge[\"catv\"] = df_merge[\"catv\"].replace([36,35,3],value=2)\n",
    "df_merge[\"catv\"] = df_merge[\"catv\"].replace([16,17,20,21],value=3)\n",
    "df_merge[\"catv\"] = df_merge[\"catv\"].replace([7,8,9],value=4)\n",
    "df_merge[\"catv\"] = df_merge[\"catv\"].replace([10,11,12],value=5)\n",
    "df_merge[\"catv\"] = df_merge[\"catv\"].replace([13,14,15],value=7)\n",
    "df_merge[\"catv\"] = df_merge[\"catv\"].replace([16,17],value=8)\n",
    "df_merge[\"catv\"] = df_merge[\"catv\"].replace([18,37,39,19,40,38],value=9)\n",
    "df_merge[\"catv\"] = df_merge[\"catv\"].replace([41,42,43],value=10)\n",
    "df_merge[\"catv\"] = df_merge[\"catv\"].replace([50,60],value=11)\n",
    "\n",
    "df_merge[\"manv\"] = df_merge[\"manv\"].replace(2,value=1)\n",
    "df_merge[\"manv\"] = df_merge[\"manv\"].replace(12,value=11)\n",
    "df_merge[\"manv\"] = df_merge[\"manv\"].replace(14,value=13)\n",
    "df_merge[\"manv\"] = df_merge[\"manv\"].replace(16,value=15)\n",
    "df_merge[\"manv\"] = df_merge[\"manv\"].replace(18,value=17)\n",
    "df_merge[\"manv\"] = df_merge[\"manv\"].replace([20,21,22,23,24,25,26],value=19)\n",
    "\n",
    "df_merge[\"obs\"] = df_merge[\"obs\"].replace([4,5],value=3)\n",
    "df_merge[\"obs\"] = df_merge[\"obs\"].replace(15,value=12)\n",
    "df_merge[\"obs\"] = df_merge[\"obs\"].replace([8,11],value=7)\n",
    "df_merge[\"obs\"] = df_merge[\"obs\"].replace(10,value=6)\n",
    "df_merge[\"obs\"] = df_merge[\"obs\"].replace(16,value=14)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corrélation entre les variables\n",
    "#plt.figure(figsize=(20,20))\n",
    "#sns.heatmap(df_merge.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on retire les variables qui ont une forte corrélation avec d'autres\n",
    "#df_merge=df_merge.drop([\"place\",\"agg\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par la suite, nous utilisons un encodage onehot sur les variables catégorielles (catégorielles nominales) qui va creer de nouvelles variables indicatrices et vont faciliter les interprétations des résultats du modèle. Aussi, nous standardisons les variables continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en utilisant une regression lasso\n",
    "numeric_var=[\"Age\",'hrmn','vma',\"nbv\"]\n",
    "lasso_x=df_merge.drop([\"grav\"],axis=1)\n",
    "categorical_var=lasso_x.drop(numeric_var, axis=1).columns\n",
    "lasso_y=df_merge[\"grav\"]\n",
    "\n",
    "numeric_pipeline = Pipeline(\n",
    "    steps=[(\"scale\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"one-hot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous déterminons ensuite le meilleur hyperparamètre alpha en utilisant une validation croisée afin que les variables qui seront sélectionner par le Lasso prédisent au mieux le niveau de gravité des accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base=categorical_pipeline.fit_transform(lasso_x.drop(columns=numeric_var,axis=1))\n",
    "base_cont=numeric_pipeline.fit_transform(lasso_x[numeric_var])\n",
    "data=pd.DataFrame(data=base, columns=categorical_pipeline.get_feature_names_out())\n",
    "data[numeric_var]=base_cont\n",
    "\n",
    "my_alphas = np.array([0.001, 0.01, 0.02, 0.025])\n",
    "lcv = LassoCV(alphas=my_alphas, fit_intercept=False, random_state=0, cv=3).fit(\n",
    "    data, lasso_y)\n",
    "print(\"alpha optimal :\", lcv.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentons enfin le Lasso, et conservons la base de données avec les variables non pénalisées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du lasso\n",
    "model = Lasso(fit_intercept=False, alpha=lcv.alpha_)\n",
    "lasso_optimal=model.fit(data, lasso_y)\n",
    "data.columns[np.abs(lasso_optimal.coef_)>0] # variables sélectionner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model=data[data.columns[np.abs(lasso_optimal.coef_)>0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implémentation de la régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Division de la base de données en données d'entrainement et données de test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(data_model,lasso_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'penalty':[\"l2\",None], 'solver':[\"newton-cg\"]}\n",
    "cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid = GridSearchCV(LogisticRegression(multi_class=\"multinomial\",class_weight=\"balanced\"), params, cv=cv)\n",
    "grid.fit(x_train, y_train)\n",
    "print(\"best params:\",grid.best_params_)\n",
    "print(\"best score:\",grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=grid.best_estimator_\n",
    "acc_train = accuracy_score(y_train, model.predict(x_train))\n",
    "acc_test = accuracy_score(y_test, model.predict(x_test))\n",
    "print(\"Training Accuracy:\", round(acc_train, 2))\n",
    "print(\"Test Accuracy:\", round(acc_test, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "from sklearn import metrics\n",
    "predictions = model.predict(x_test)\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(acc_test)\n",
    "#plt.title(all_sample_title, size = 15);\n",
    "#plt.savefig('toy_Digits_ConfusionSeabornCodementor.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances1 = model.coef_[0]\n",
    "odds_ratios1 = pd.Series(np.exp(importances1), index=data_model.columns).sort_values() #coefficient des variables dans le modèle\n",
    "fig1 = px.bar(x=odds_ratios1.tail(15),y=odds_ratios1.tail(15).index, orientation=\"h\",title=\"Odds Ratio-reference 1\")\n",
    "fig1.update_layout(xaxis_title=\"exp(coef)\", yaxis_title=\"Variables\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances2 = model.coef_[1]\n",
    "odds_ratios2 = pd.Series(np.exp(importances2), index=data_model.columns).sort_values() #coefficient des variables dans le modèle\n",
    "fig2 = px.bar(x=odds_ratios2.tail(15),y=odds_ratios2.tail(15).index, orientation=\"h\",title=\"Odds Ratio-reference 2\")\n",
    "fig2.update_layout(xaxis_title=\"exp(coef)\", yaxis_title=\"Variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L'utilisation de la ceinture de sécurité seule augmente la chance que la ravité de l'accident soit moyenne de 82%.\n",
    "- Lorsque la voiture se trouve entre deux files avant l'accident, la gravité de l'accident à 54% d'être moyenne que d'être faible\n",
    "- un véhicule scooter<50cm3, l'autoroute, heurté une glissière en béton augmentent la chance d'avoir un accident de gravité moyenne de 54%,51%,47% respectivement.\n",
    "- une collision de trois véhicules et plus-en chaine augmentent la chance d'avoir un accident de gravité moyenne de 45%.\n",
    " - les caractéristiques suivantes: une manoeuvre d'évitement, un accident sur piste cyclable, heurté un véhicule en stationnement, avec un scooter > 50cm3 et <=125cm3, avoir un accident sur la chaussée, ou avec un véhicule léger, ou encore sous une pluie légère,  augmentent la chance d'avoir un accident de gravité moyenne entre 15% et 30%. \n",
    "\n",
    " - Avoir un accident sur un cyclomoteur<50cm3 ou sur une motocyclette augmente la probabilité que l'accident soit grave de plus de 40%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = px.bar(x=odds_ratios1.head(10),y=odds_ratios1.head(10).index, orientation=\"h\",title=\"Odds Ratio-reference 1\")\n",
    "fig3.update_layout(xaxis_title=\"exp(coef)\", yaxis_title=\"Variables\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4 = px.bar(x=odds_ratios2.head(10),y=odds_ratios2.head(10).index, orientation=\"h\",title=\"Odds Ratio-reference 2\")\n",
    "fig4.update_layout(xaxis_title=\"exp(coef)\", yaxis_title=\"Variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation d'un modèle de random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "n_estimators = [int(x) for x in np.arange(100, 500, 100)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [10,15]\n",
    "min_samples_split = [2, 5]\n",
    "min_samples_leaf = [1, 2]\n",
    "criterion = ['gini', 'entropy']\n",
    "params_grid = {'n_estimators': n_estimators, \n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'criterion': criterion\n",
    "              }\n",
    "\n",
    "cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid = GridSearchCV(RandomForestClassifier(class_weight=\"balanced\"), params_grid, cv=cv)\n",
    "grid.fit(x_train, y_train)\n",
    "print(\"best params:\",grid.best_params_)\n",
    "print(\"best score:\",grid.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
